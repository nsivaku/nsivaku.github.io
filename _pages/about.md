---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am a 4th year undergraduate student majoring in Computer Science and Economics with a minor in Statistics at UNC Chapel Hill. I work in the [MURGe Lab](https://murgelab.cs.unc.edu/) where I am mentored by [Prof. Mohit Bansal](https://www.cs.unc.edu/~mbansal/) and [Prof. Elias Stengel-Eskin](https://esteng.github.io/). **I am currently seeking PhD programs for Fall 2026.** 

My research focuses on building collaborative multimodal AI systems for monitorable LLM reasoning. Additionally, I am interested in using post-training methods for LLMs to improve interpretability.

# ğŸ”¥ News
- *2025.12*: &nbsp;ğŸ‰ğŸ‰ Honored to receive an Honorable Mention for the CRA Outstanding Undergraduate Research Award.
- *2025.12*: &nbsp;ğŸ‰ğŸ‰ New preprint "Movie Facts and Fibs (MF^2): A Benchmark for Long Movie Understanding" introducing a benchmark for narrative understanding of long open-domain movies.
- *2025.08*: &nbsp;ğŸ‰ğŸ‰ Our paper "A Multimodal Classroom Video Question-Answering Framework for Automated Understanding of Collaborative Learning" was accepted to ICMI 2025!
- *2025.06*: &nbsp;ğŸ‰ğŸ‰ New preprint "Movie Facts and Fibs (MF^2): A Benchmark for Long Movie Understanding" introducing a benchmark for narrative understanding of long open-domain movies.

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/dart.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://www.arxiv.org/abs/2512.07132)

**Nithin Sivakumaran**, Justin Chih-Yao Chen, David Wan, Yue Zhang, Jaehong Yoon, Elias Stengel-Eskin, Mohit Bansal.

[**Code**](https://github.com/nsivaku/dart) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose DART, a multi-agent multimodal debate framework that uses disagreement between VLM agents to address visual uncertainty
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICMI 2025</div><img src='images/engagevp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Multimodal Classroom Video Question-Answering Framework for Automated Understanding of Collaborative Learning](https://dl.acm.org/doi/abs/10.1145/3716553.3750795)

**Nithin Sivakumaran***, Chia-Yu Yang*, Abhay Zala*, Shoubin Yu, Daeun Hong, Xiaotian Zou, Elias Stengel-Eskin, Dan Carpenter, Wookhee Min, Cindy Hmelo-Silver, Jonathan Rowe, James Lester, Mohit Bansal.

[**Code**](https://github.com/nsivaku/EngageVP) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose EngageVP, a new mutimodal video QA framework for stronger understanding of student engagement and behavior in classroom videos
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/MF2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Movie Facts and Fibs (MF^2): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)

Emmanouil Zaranis, AntÃ³nio Farinhas, Saul Santos, Beatriz Canaverde,...**Nithin Sivakumaran**, et al.

[**Code**](https://github.com/deep-spin/MF2) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose MF2, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information from full-length movies
</div>
</div>

# ğŸ’» Experience
- *2024.05 - 2024.08*, [NSF EngageAI Institute](https://engageai.org/), Research Intern.
- *2023.05 - 2023.08*, [Principal Financial Group](https://www.principal.com/), Software Engineering Intern.
